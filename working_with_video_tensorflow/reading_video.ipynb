{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2792f1-5eee-430d-a1d1-64f59a95e7ee",
   "metadata": {},
   "source": [
    "# Introduction to Video Classification and Human Avtivity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df4c02-6f15-424f-adad-3bca019f663d",
   "metadata": {},
   "source": [
    "This notebook contains the code from [this tutorial](https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/) by Taha Anwar.\n",
    "\n",
    "The goal of completing this tutorial is to learn:\n",
    "1. Video Preprocessing\n",
    "2. How to input video data to neural network?\n",
    "3. Video Classification\n",
    "\n",
    "This tutorial is a prerequisite to [this tensorflow tutorial](https://youtu.be/DjQFwJGnRDY?list=PLQY2H8rRoyvwmjfn7hM-Yg_6RIyoMnKQx) by Shilpa Kancharla.\n",
    "\n",
    "In this tutorial we will go over a number of approaches to make a video classifier for human activity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c125d-d0cb-4b79-b299-e6a9370110bf",
   "metadata": {},
   "source": [
    "## 1. Understanding Human Activity Recognition\n",
    "\n",
    "- **Task:** Classifying or prediction the activity/action being performed by someone is called Activity Recognition.\n",
    "\n",
    "- How is it different from a normal classification task? \\\n",
    "For human activity recognition, you need a series of data points to predict the action being performed correctly. You can't just give prediction from frame to frame. But the information of a series of frames is required. **So, it is a time-series classification problem.** Hence, you need data from a series of timesteps.\n",
    "\n",
    "- How was Human Activity Recognition traditionally solved? \n",
    "  1. (Most common and effective technique) Attach a wearable sensor (example a smartphone) on to a person and then train a temporal model like an LSTM on the output of sensor data. \\\n",
    "  Here the readings from accelerometer and gyroscope are used to train a model which outputs these six classes: (walking, walking upstairs, walking downstairs, sitting, standingm laying)\n",
    "\n",
    "  2. Image Classification \n",
    "     1. Through simple image classification on frame by frame basis. This can also work well because the model learns environmental context as well. Check the tutorial webpage for explanation.\n",
    "     2. Simple image classification on frame by frame basis, but results averaged over a number of frames. Moving average.\n",
    "\n",
    "        \n",
    "  3. Video Classification \\\n",
    "     We are looking at methods which can take input, a short video clip and then output the activity being performed.\n",
    "     1. Method1: Single-Frame CNN, then averaging\n",
    "     2. Method2: Late Fusion\n",
    "     3. Method3: Early Fusion\n",
    "     4. Method4: Using CNN with LSTM\n",
    "     5. Method5: Using pose detection and LSTM\n",
    "     6. Method6: Using Optical Flow and CNN\n",
    "     7. Method7: Using SlowFast Networks\n",
    "     8. Method8: Using 3D CNN's / Slow Fusion\n",
    "    \n",
    "     \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7b71c-210c-42ed-b7db-cf95fb7cf9d0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ada4d-cbe7-4965-81aa-9d30ee11d641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025226d1-6d1f-41b5-a649-953c519d795c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa49ad2-583a-4501-8eae-c29c0e5d6d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13a3d2-6528-4ac5-9900-4ccb3be874fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ef6c4-b194-4e47-8f41-de8c96bfa638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a91c1-0866-4f59-bbd9-20b5e8a204df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e97f2e-3484-4194-b0a3-e77ab004aef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
